{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd #引入Pandas模組\n",
    "import csv\n",
    "\n",
    "df=pd.read_csv('./file/ptt_dcard.csv',encoding='utf-8')\n",
    "#encoding='windows-1252' -> 不加的話，會跑出下面的錯誤\n",
    "#'utf-8' codec can't decode byte 0xa6 in position 0: invalid start byte\n",
    "\n",
    "#1.刪掉不要的column\n",
    "df.drop(labels=['resorce','title','author','url'],axis='columns',inplace=True)\n",
    "#inplace=True 改變原本的檔案\n",
    "\n",
    "#2.更換column的順序\n",
    "df = df[['content', 'tag']]\n",
    "\n",
    "#3.刪掉不要的tag 3 與 4\n",
    "df=df[ df['tag'] < 3] #留下tag小於3的資料，因為我們要的tag是1與2\n",
    "df.to_csv('for_csdn_1and2.csv',encoding='utf-8') #把結果存起來\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import nltk\n",
    "import codecs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12)\t0.08638684255813601\n",
      "  (0, 9)\t0.08638684255813601\n",
      "  (0, 5)\t0.08638684255813601\n",
      "  (0, 6)\t0.08638684255813601\n",
      "  (0, 8)\t0.08638684255813601\n",
      "  (0, 7)\t0.08638684255813601\n",
      "  (0, 100)\t0.08638684255813601\n",
      "  (0, 64)\t0.08638684255813601\n",
      "  (0, 33)\t0.08638684255813601\n",
      "  (0, 75)\t0.08638684255813601\n",
      "  (0, 76)\t0.08638684255813601\n",
      "  (0, 112)\t0.08638684255813601\n",
      "  (0, 85)\t0.08638684255813601\n",
      "  (0, 36)\t0.08638684255813601\n",
      "  (0, 95)\t0.08638684255813601\n",
      "  (0, 38)\t0.08638684255813601\n",
      "  (0, 17)\t0.08638684255813601\n",
      "  (0, 4)\t0.08638684255813601\n",
      "  (0, 39)\t0.08638684255813601\n",
      "  (0, 25)\t0.08638684255813601\n",
      "  (0, 48)\t0.08638684255813601\n",
      "  (0, 82)\t0.08638684255813601\n",
      "  (0, 118)\t0.17277368511627203\n",
      "  (0, 20)\t0.08638684255813601\n",
      "  (0, 3)\t0.08638684255813601\n",
      "  :\t:\n",
      "  (0, 110)\t0.08638684255813601\n",
      "  (0, 42)\t0.08638684255813601\n",
      "  (0, 113)\t0.08638684255813601\n",
      "  (0, 94)\t0.08638684255813601\n",
      "  (0, 115)\t0.08638684255813601\n",
      "  (0, 79)\t0.08638684255813601\n",
      "  (0, 103)\t0.25916052767440806\n",
      "  (0, 19)\t0.08638684255813601\n",
      "  (0, 99)\t0.08638684255813601\n",
      "  (0, 59)\t0.08638684255813601\n",
      "  (0, 68)\t0.08638684255813601\n",
      "  (0, 14)\t0.08638684255813601\n",
      "  (0, 71)\t0.08638684255813601\n",
      "  (0, 21)\t0.08638684255813601\n",
      "  (0, 70)\t0.08638684255813601\n",
      "  (0, 50)\t0.08638684255813601\n",
      "  (0, 55)\t0.08638684255813601\n",
      "  (0, 74)\t0.08638684255813601\n",
      "  (0, 61)\t0.08638684255813601\n",
      "  (0, 102)\t0.08638684255813601\n",
      "  (0, 15)\t0.08638684255813601\n",
      "  (0, 67)\t0.08638684255813601\n",
      "  (0, 116)\t0.08638684255813601\n",
      "  (0, 37)\t0.08638684255813601\n",
      "  (0, 35)\t0.08638684255813601: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#加入stop word\n",
    "with open(file='./mydict/stop_words.txt', mode='r', encoding='utf-8') as file:\n",
    "    stop_words = file.read().split('\\n')\n",
    "    \n",
    "stop_words.append('\\n')  ## 文章中有許多分行符號，這邊加入停用字中，可以把它拿掉\n",
    "stop_words.append('\\n\\n')\n",
    "\n",
    "def jieba_tokenize(new_line):\n",
    "    seg_stop_words_list = []\n",
    "    seg_words_list = jieba.lcut(new_line)\n",
    "    for term in seg_words_list:\n",
    "        if term not in stop_words:\n",
    "            seg_stop_words_list.append(term)\n",
    "#seg_stop_words_list\n",
    "    return seg_stop_words_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=jieba_tokenize, lowercase=False)\n",
    "'''\n",
    "tokenizer: 指定分词函数\n",
    "lowercase: 在分词之前将所有的文本转换成小写，因为涉及到中文文本处理，\n",
    "所以最好是False\n",
    "'''\n",
    "#需要进行聚类的文本集\n",
    "#text_list = [\"今天天气真好啊啊啊啊\", \"我早上8点起床的\", \\\n",
    "#\"我今天拿到了Google的Offer\", \"清华大学在自然语言处理方面真厉害\"]\n",
    "\n",
    "#载入数据\n",
    "#要使用絕對路徑,不然無法開啟\n",
    "rows=pd.read_csv('C:/Users/leavi/Desktop/pyETL/file/final_for_svc.csv', header=0,encoding='utf-8',dtype=str)\n",
    "\n",
    "text_list=[str(rows['content'])]\n",
    " \n",
    "    \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_list)\n",
    " \n",
    "k = 2\n",
    "\n",
    "'''\n",
    "n_clusters: 指定K的值\n",
    "max_iter: 对于单次初始值计算的最大迭代次数\n",
    "n_init: 重新选择初始值的次数\n",
    "init: 制定初始值选择的算法\n",
    "n_jobs: 进程个数，为-1的时候是指默认跑满CPU\n",
    "注意，这个对于单个初始值的计算始终只会使用单进程计算，\n",
    "并行计算只是针对与不同初始值的计算。比如n_init=10，n_jobs=40, \n",
    "服务器上面有20个CPU可以开40个进程，最终只会开10个进程\n",
    "'''\n",
    "\n",
    "\n",
    "for col in tfidf_matrix:\n",
    "    kmeans = KMeans(n_clusters=k,max_iter=300, n_init=10,init='k-means++')\n",
    "    X = tfidf_matrix.reshape(-1, 1)\n",
    "    kmeans.fit(X)\n",
    "    print(\"{}: {}\".format(col, kmeans.predict(X)))\n",
    "\n",
    "\n",
    "#返回各自文本的所被分配到的类索引\n",
    "    #result = km_cluster.fit_predict(X)\n",
    " \n",
    "    #print(\"Predicting result: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
