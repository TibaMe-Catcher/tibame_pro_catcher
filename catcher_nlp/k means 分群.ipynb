{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd #引入Pandas模組\n",
    "import csv\n",
    "\n",
    "df=pd.read_csv('./file/ptt_dcard.csv',encoding='utf-8')\n",
    "#encoding='windows-1252' -> 不加的話，會跑出下面的錯誤\n",
    "#'utf-8' codec can't decode byte 0xa6 in position 0: invalid start byte\n",
    "\n",
    "#1.刪掉不要的column\n",
    "df.drop(labels=['resorce','title','author','url'],axis='columns',inplace=True)\n",
    "#inplace=True 改變原本的檔案\n",
    "\n",
    "#2.更換column的順序\n",
    "df = df[['content', 'tag']]\n",
    "\n",
    "#3.刪掉不要的tag 3 與 4\n",
    "df=df[ df['tag'] < 3] #留下tag小於3的資料，因為我們要的tag是1與2\n",
    "df.to_csv('for_csdn_1and2.csv',encoding='utf-8') #把結果存起來\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import nltk\n",
    "import codecs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\leavi\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.616 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12)\t0.08638684255813601\n",
      "  (0, 9)\t0.08638684255813601\n",
      "  (0, 5)\t0.08638684255813601\n",
      "  (0, 6)\t0.08638684255813601\n",
      "  (0, 8)\t0.08638684255813601\n",
      "  (0, 7)\t0.08638684255813601\n",
      "  (0, 100)\t0.08638684255813601\n",
      "  (0, 64)\t0.08638684255813601\n",
      "  (0, 33)\t0.08638684255813601\n",
      "  (0, 75)\t0.08638684255813601\n",
      "  (0, 76)\t0.08638684255813601\n",
      "  (0, 112)\t0.08638684255813601\n",
      "  (0, 85)\t0.08638684255813601\n",
      "  (0, 36)\t0.08638684255813601\n",
      "  (0, 95)\t0.08638684255813601\n",
      "  (0, 38)\t0.08638684255813601\n",
      "  (0, 17)\t0.08638684255813601\n",
      "  (0, 4)\t0.08638684255813601\n",
      "  (0, 39)\t0.08638684255813601\n",
      "  (0, 25)\t0.08638684255813601\n",
      "  (0, 48)\t0.08638684255813601\n",
      "  (0, 82)\t0.08638684255813601\n",
      "  (0, 118)\t0.17277368511627203\n",
      "  (0, 20)\t0.08638684255813601\n",
      "  (0, 3)\t0.08638684255813601\n",
      "  :\t:\n",
      "  (0, 110)\t0.08638684255813601\n",
      "  (0, 42)\t0.08638684255813601\n",
      "  (0, 113)\t0.08638684255813601\n",
      "  (0, 94)\t0.08638684255813601\n",
      "  (0, 115)\t0.08638684255813601\n",
      "  (0, 79)\t0.08638684255813601\n",
      "  (0, 103)\t0.25916052767440806\n",
      "  (0, 19)\t0.08638684255813601\n",
      "  (0, 99)\t0.08638684255813601\n",
      "  (0, 59)\t0.08638684255813601\n",
      "  (0, 68)\t0.08638684255813601\n",
      "  (0, 14)\t0.08638684255813601\n",
      "  (0, 71)\t0.08638684255813601\n",
      "  (0, 21)\t0.08638684255813601\n",
      "  (0, 70)\t0.08638684255813601\n",
      "  (0, 50)\t0.08638684255813601\n",
      "  (0, 55)\t0.08638684255813601\n",
      "  (0, 74)\t0.08638684255813601\n",
      "  (0, 61)\t0.08638684255813601\n",
      "  (0, 102)\t0.08638684255813601\n",
      "  (0, 15)\t0.08638684255813601\n",
      "  (0, 67)\t0.08638684255813601\n",
      "  (0, 116)\t0.08638684255813601\n",
      "  (0, 37)\t0.08638684255813601\n",
      "  (0, 35)\t0.08638684255813601: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#加入stop word\n",
    "with open(file='./mydict/stop_words.txt', mode='r', encoding='utf-8') as file:\n",
    "    stop_words = file.read().split('\\n')\n",
    "    \n",
    "stop_words.append('\\n')  ## 文章中有許多分行符號，這邊加入停用字中，可以把它拿掉\n",
    "stop_words.append('\\n\\n')\n",
    "\n",
    "def jieba_tokenize(new_line):\n",
    "    seg_stop_words_list = []\n",
    "    seg_words_list = jieba.lcut(new_line)\n",
    "    for term in seg_words_list:\n",
    "        if term not in stop_words:\n",
    "            seg_stop_words_list.append(term)\n",
    "#seg_stop_words_list\n",
    "    return seg_stop_words_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=jieba_tokenize, lowercase=False)\n",
    "'''\n",
    "tokenizer: 指定分词函数\n",
    "lowercase: 在分词之前将所有的文本转换成小写，因为涉及到中文文本处理，\n",
    "所以最好是False\n",
    "'''\n",
    "#需要进行聚类的文本集\n",
    "#text_list = [\"今天天气真好啊啊啊啊\", \"我早上8点起床的\", \\\n",
    "#\"我今天拿到了Google的Offer\", \"清华大学在自然语言处理方面真厉害\"]\n",
    "\n",
    "#载入数据\n",
    "#要使用絕對路徑,不然無法開啟\n",
    "rows=pd.read_csv('C:/Users/leavi/Desktop/pyETL/file/final_for_svc.csv', header=0,encoding='utf-8',dtype=str)\n",
    "\n",
    "text_list=[str(rows['content'])]\n",
    " \n",
    "    \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_list)\n",
    " \n",
    "k = 2\n",
    "\n",
    "'''\n",
    "n_clusters: 指定K的值\n",
    "max_iter: 对于单次初始值计算的最大迭代次数\n",
    "n_init: 重新选择初始值的次数\n",
    "init: 制定初始值选择的算法\n",
    "n_jobs: 进程个数，为-1的时候是指默认跑满CPU\n",
    "注意，这个对于单个初始值的计算始终只会使用单进程计算，\n",
    "并行计算只是针对与不同初始值的计算。比如n_init=10，n_jobs=40, \n",
    "服务器上面有20个CPU可以开40个进程，最终只会开10个进程\n",
    "'''\n",
    "\n",
    "\n",
    "for col in tfidf_matrix:\n",
    "    kmeans = KMeans(n_clusters=k,max_iter=300, n_init=10,init='k-means++')\n",
    "    X = tfidf_matrix.reshape(-1, 1)\n",
    "    kmeans.fit(X)\n",
    "    print(\"{}: {}\".format(col, kmeans.predict(X)))\n",
    "\n",
    "\n",
    "#返回各自文本的所被分配到的类索引\n",
    "    #result = km_cluster.fit_predict(X)\n",
    " \n",
    "    #print(\"Predicting result: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b749286ca416>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mrows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhero\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhero\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\catcher_nlp\\lib\\site-packages\\texthero\\representation.py\u001b[0m in \u001b[0;36mtfidf\u001b[1;34m(s, max_features, min_df, return_feature_names)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     )\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_feature_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\catcher_nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \"\"\"\n\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\catcher_nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\catcher_nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\catcher_nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\catcher_nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    220\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "import texthero as hero\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "df['tfidf'] = (\n",
    "    rows['content']\n",
    "    .pipe(hero.clean)\n",
    "    .pipe(hero.tfidf(rows['content'],max_features=100))\n",
    ")\n",
    "\n",
    "df['kmeans_labels'] = (\n",
    "    df['tfidf']\n",
    "    .pipe(hero.kmeans, n_clusters=2)\n",
    "    .astype(str)\n",
    ")\n",
    "\n",
    "df['pca'] = df['tfidf'].pipe(hero.pca)\n",
    "\n",
    "hero.scatterplot(df, 'pca', color='kmeans_labels', title=\"K-means BBC Sport news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
