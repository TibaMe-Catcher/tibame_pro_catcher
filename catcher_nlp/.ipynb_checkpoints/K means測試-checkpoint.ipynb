{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "jieba.load_userdict('./mydict/mydict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料\n",
    "fileAllLines = []\n",
    "with open('./file/test.txt','r',encoding=\"utf-8\") as fileLine:\n",
    "    for line in iter(lambda: fileLine.read(1024), ''):\n",
    "        #print(line)\n",
    "        fileAllLines.append(line)\n",
    "        \n",
    "new_line = ' '.join(fileAllLines)\n",
    "    \n",
    "    \n",
    "# 讀取 stop words\n",
    "with open(file='./mydict/stop_words.txt', mode='r', encoding='utf-8') as file:\n",
    "    stop_words = file.read().split('\\n')\n",
    "    \n",
    "stop_words.append('\\n')  ## 文章中有許多分行符號，這邊加入停用字中，可以把它拿掉\n",
    "stop_words.append('\\n\\n')\n",
    "#stop_words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除繁體中文以外的英文、數字、符號\n",
    "rule = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "\n",
    "#進行匯入停止字的分詞\n",
    "seg_stop_words_list = []\n",
    "seg_words_list = jieba.lcut(new_line)\n",
    "for term in seg_words_list:\n",
    "    if term not in stop_words:\n",
    "        seg_stop_words_list.append(term)\n",
    "#seg_stop_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def cal_dist(self, p0, p1):\n",
    "        \"\"\"\n",
    "        比較兩點的距離\n",
    "        \"\"\"\n",
    "        return np.sqrt(np.sum((p0-p1)**2))\n",
    "    \n",
    "    def nearest_cluster_center(self, point, cluster_centers):\n",
    "        \"\"\"\n",
    "        找到距離 point 最近的中心點\n",
    "        \"\"\"\n",
    "        min_dist = float(\"inf\")\n",
    "        m = cluster_centers.shape[0]\n",
    "        for i in range(m):\n",
    "            d = self.cal_dist(point, cluster_centers[i])\n",
    "            if min_dist > d:\n",
    "                min_dist = d\n",
    "        return min_dist \n",
    "\n",
    "    def get_centroids(self, datapoints, k):\n",
    "        \"\"\"\n",
    "        K-means++ 演算法，取得初始化中心點\n",
    "        \"\"\"\n",
    "        clusters = np.array([random.choice(datapoints)])\n",
    "        dist = np.zeros(len(datapoints))\n",
    "        \n",
    "        for i in range(k-1):\n",
    "            sum_dist = 0\n",
    "            for j, point in enumerate(datapoints):\n",
    "                dist[j] = self.nearest_cluster_center(point, clusters)\n",
    "                sum_dist += dist[j]\n",
    "            \n",
    "            sum_dist *= random.random()\n",
    "            for j, d in enumerate(dist):\n",
    "                sum_dist = sum_dist - d\n",
    "                if sum_dist <= 0:\n",
    "                    clusters = np.append(clusters, [datapoints[j]], axis=0)\n",
    "                    break\n",
    "        \n",
    "        return clusters\n",
    "        \n",
    "        \n",
    "    def kmeans_plus_plus(self, datapoints, k=2):\n",
    "        \"\"\"\n",
    "        K-means 演算法\n",
    "        \"\"\"\n",
    "        # 定義資料維度\n",
    "        d = datapoints.shape[1]\n",
    "        # 最大的迭代次數\n",
    "        Max_Iterations = 100\n",
    "\n",
    "        cluster = np.zeros(datapoints.shape[0])\n",
    "        prev_cluster = np.ones(datapoints.shape[0])\n",
    "\n",
    "        cluster_centers = self.get_centroids(datapoints, k)\n",
    "\n",
    "        iteration = 0\n",
    "        while np.array_equal(cluster, prev_cluster) is False or iteration > Max_Iterations:\n",
    "            iteration += 1\n",
    "            prev_cluster = cluster.copy()\n",
    "\n",
    "            # 將每一個點做分群\n",
    "            for idx, point in enumerate(datapoints):\n",
    "                min_dist = float(\"inf\")\n",
    "                for c, cluster_center in enumerate(cluster_centers):\n",
    "                    dist = self.cal_dist(point, cluster_center)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist  \n",
    "                        cluster[idx] = c   # 指定該點屬於哪個分群\n",
    "\n",
    "            # 更新分群的中心\n",
    "            for k in range(len(cluster_centers)):\n",
    "                new_center = np.zeros(d)\n",
    "                members = 0\n",
    "                for point, c in zip(datapoints, cluster):\n",
    "                    if c == k:\n",
    "                        new_center += point\n",
    "                        members += 1\n",
    "                if members > 0:\n",
    "                    new_center = new_center / members\n",
    "                cluster_centers[k] = new_center\n",
    "\n",
    "        return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 使用 tf-idf 向量化\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(seg_stop_words_list)\n",
    "tfidf = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: 第一次 發文 手機 排版 請見諒 患有 憂鬱症 存在 感到 餘 不知道 活著 意義是 每天 行屍 走 肉 活著 不斷 嘲笑 發音 問題 長 大些 嘲笑 長 相 想 結束 站 陽台 吹 吹 風 不行 認為 想 跳下去 然後就 抓 回來 打算 站 陽台 圍牆 想 跳下去 是否 死 想到 會嚇 許多人 作罷 話 想 吞藥來 結束 心態 扭曲 造成 可逆 心理 傷害 憂鬱症 恐慌 症\n",
      "Cluster 1: 我四歲\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "Kmeans_cluster = KMeans()\n",
    "speech_cluster_result = Kmeans_cluster.kmeans_plus_plus(tfidf, k)\n",
    "cluster = [[] for _ in range(k)]\n",
    "\n",
    "for idx, c in enumerate(speech_cluster_result):\n",
    "    cluster[int(c)].append(seg_stop_words_list[idx])\n",
    "    \n",
    "for c, result in enumerate(cluster):\n",
    "    print('Cluster {}: {}'.format(c, ' '.join(result))[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
